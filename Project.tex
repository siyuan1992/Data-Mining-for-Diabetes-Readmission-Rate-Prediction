\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\ErrorTok}[1]{\textbf{{#1}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Siyuan Meng; siyuanme@usc.edu},
            pdftitle={Data Mining for Diabetes Readmission Rate Prediction},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Data Mining for Diabetes Readmission Rate Prediction}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Siyuan Meng \\ \href{mailto:siyuanme@usc.edu}{\nolinkurl{siyuanme@usc.edu}}}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{December 8, 2015}



\begin{document}

\maketitle


\section{Project Homepage}\label{project-homepage}

The link of project repository is
\url{https://github.com/siyuan1992/EE660_Project}. For more information,
please see \texttt{README.md}.

\section{Abstract}\label{abstract}

The management of hyperglycemia in the hospitalized inpatient becomes
increasing recognized, due to the morbidity and mortality outcome
{[}1,2{]}. For most non-ICU (Inensive Care Unit) patients, anecdotal
evidence says that inpatient managenent is arbitrary and often leads to
either no treatment or wide fluctuations in glucose when traditional
management strategies are employed {[}4,5{]}. Thus, protocols are
recommended. Effective prediction on readmissions enables hospitals to
identify and target patients at high risk {[}slideshare{]}. Therefore,
the goals are discovering major factors contributing to hospitals
readmissions as well as finding the effective method to predict the type
of readmissions. Top three most important features for prediction are
\texttt{Number of inpatient visits}, \texttt{Admission type} and
\texttt{Admission source}. The best method for classification is
boosting-tree classifier. However, with data pre-mining and QUEST
(Quick, Unbiased and Effecient Statistical Tree), there will be
5\%\textasciitilde{}7\% further accuray improvement.

\section{Problem Statement and Goals}\label{problem-statement-and-goals}

The data is from the Center for Clinical and Translational Research,
Viginia Commonwealth University, which presents 10 years (1999-2008) of
clinical care at 130 US hospitals and integrated delivery networks
{[}paper{]}. The goals here are identify the major factors that
contribute to hospital readmissions as well as find the best model to
predict the readmission type given inpatient information.

The difficulties of this problem are mainly in the following aspects:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The dataset is relatively large (\textasciitilde{}100,000 samples, 55
  features)
\item
  Several features have large propotional missing data due to the fact
  that prior to the HITECH legislation of the American Reinvestment and
  Recovery Act in 2009 hospitals and clinics were not required to
  capture it in a structured format {[}paper{]}.
\item
  Significant amounts of preprocessing required:

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    The preliminary dataet contained multiple inpaitent visits for some
    patients and the observations could not be considered statistically
    independent {[}paper{]}.
  \item
    Even though the preliminary dataset was extracted from the database
    with processing, lots of redundant features still exist.
  \item
    Almost 80\% features are nominal. Thus, after feature encoding, data
    will be expanded into a large dimension, which makes feature
    selection and classification extremely time consuming.
  \item
    Class labels are hierarchical. Three types of readmission are
    \texttt{NO}, \texttt{\textgreater{}30}, \texttt{\textless{}30},
    i.e., whether a patient will have readmission and if the patient
    have, a hospitalization occurs within 30 days or not.
  \end{itemize}
\end{itemize}

\section{Literature Review}\label{literature-review}

The research artical {[}paper{]} talked about the background information
about th dataset and the information of each feature, which as a
reference, helps me to deal with redundant features.

The other paper {[}slide{]} analyze the influence of different features
and use some tricks to improve the performance. The trick they used are
listed here:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Data pre-mining

  \begin{itemize}
  \itemsep1pt\parskip0pt\parsep0pt
  \item
    Re-categorize readmission group from 3 groups to 2 groups
    (Readmission \textless{}30 days and Non-within 30 days.)
  \item
    Subsample to make readmission group's ratio be 1:1
  \item
    Review each feature's relationship with readmission
  \item
    Review numeric variables correlation
  \end{itemize}
\item
  Use QUEST model to improve accuracy.
\end{itemize}

Several gruops use the similar trick, like recategorizing readmission
groups to two (Readmission or not). However, data pre-mining is of
upmost importance in improving the model accuracy. With data pre-mining
and ensemble models, the highest accuracy could reach 80\% {[}2
stage{]}.

\section{Prior and Related Work}\label{prior-and-related-work}

This project is exclusively for EE660.

\section{Project Formulation and
Setup}\label{project-formulation-and-setup}

I tried several modals and algorithms for this project, e.g.~Perceptron,
SVM, logistic classifier, etc. I want to list two of them which have
best performances.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The first is gradient tree boosting classifier. The algorithm
  (pseudocode) is as follows {[}wiki{]}:
\end{itemize}

\begin{longtable}[c]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
Input: training set \({(\textbf{x}_{i},y_{i})}_{i=1}^N\), loss function
\(L(y,f(\textbf{x}))\) and number of iteration M.
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
Algorithm:
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
I. Initialize model with a constant value:
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
\[f_{0}(x)=\mathop{\arg\min}\limits_{\gamma}\sum_{i=1}^NL(y_i,\gamma)\].
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
II. for m=1 to M:
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
1. Compute pesudo-residuals:
\[r_{im}=-[\frac{\partial{L(y_i,f(\textbf{x}_i))}}{\partial{f(\textbf{x}_i)}}]_{f(\textbf{x})=f_{m-1}(\textbf{x})} \qquad  for\ i=1,...,N\].
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
2. Train with training set \({(\textbf{x}_i,r_{im})}_{i=1}^N\).
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
3. Compute \(\gamma{_m}\) by solving:
\[\gamma{_m}=\mathop{\arg\min}\limits_{\gamma}\sum_{i=1}^NL(y_i,f_{m-1}(\textbf{x}_i)+\gamma{h_m(\textbf{x}_i)})\].
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
4. Update the model:
\[f_m(\textbf{x})=f_{m-1}(\textbf{x})+\gamma{_m}h_m(\textbf{x})\]
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.97\columnwidth}\raggedright\strut
III. Output \(F_M(\textbf{x})\).
\strut\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\section{Reproducibility}\label{reproducibility}

In order to get the same results, need certain set of packages, as well
as setting a pseudo-random seed equal the one I used.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  The following libraries were used for this project:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: lattice
## Loading required package: ggplot2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## randomForest 4.6-12
## Type rfNews() to see new features/changes/bug fixes.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pander)}
\KeywordTok{library}\NormalTok{(psych)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Attaching package: 'psych'
## 
## The following object is masked from 'package:randomForest':
## 
##     outlier
## 
## The following object is masked from 'package:ggplot2':
## 
##     %+%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(FactoMineR)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Here is the seed I set to generate pseudo-random numbers for spliting
  training and test dataset. (see \texttt{Preprocessing} section)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Getting data}\label{getting-data}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{setwd}\NormalTok{(}\StringTok{"~/Documents/2015Fall/EE660/EE660_Project"}\NormalTok{)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"~/Documents/2015Fall/EE660/EE660_Project/diabetic_data.csv"}\NormalTok{,}
                 \DataTypeTok{stringsAsFactors=}\NormalTok{T,}\DataTypeTok{na.strings =} \StringTok{'?'}\NormalTok{)}
\KeywordTok{dim}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 101766     50
\end{verbatim}

\section{Cleaning data}\label{cleaning-data}

The original missing value information can be found in
\url{http://www.hindawi.com/journals/bmri/2014/781670/tab1/}. For
simplicity, only show features which have missing values. (cite)

\begin{longtable}[c]{@{}cccc@{}}
\toprule
\begin{minipage}[b]{0.20\columnwidth}\centering\strut
Feature\_name
\strut\end{minipage} &
\begin{minipage}[b]{0.09\columnwidth}\centering\strut
Type
\strut\end{minipage} &
\begin{minipage}[b]{0.35\columnwidth}\centering\strut
Discription
\strut\end{minipage} &
\begin{minipage}[b]{0.24\columnwidth}\centering\strut
Propotional\_missing
\strut\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.20\columnwidth}\centering\strut
Race
\strut\end{minipage} &
\begin{minipage}[t]{0.09\columnwidth}\centering\strut
Nominal
\strut\end{minipage} &
\begin{minipage}[t]{0.35\columnwidth}\centering\strut
Values: Caucasian, Asian, African American, Hispanic, and other
\strut\end{minipage} &
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
2\%
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.20\columnwidth}\centering\strut
Weight
\strut\end{minipage} &
\begin{minipage}[t]{0.09\columnwidth}\centering\strut
Numeric
\strut\end{minipage} &
\begin{minipage}[t]{0.35\columnwidth}\centering\strut
Weight in pounds
\strut\end{minipage} &
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
97\%
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.20\columnwidth}\centering\strut
Payer code
\strut\end{minipage} &
\begin{minipage}[t]{0.09\columnwidth}\centering\strut
Nominal
\strut\end{minipage} &
\begin{minipage}[t]{0.35\columnwidth}\centering\strut
Integer identifier corresponding to 23 distinct values, for example,
Blue Cross/Blue Shield, Medicare, and self-pay
\strut\end{minipage} &
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
40\%
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.20\columnwidth}\centering\strut
Medical specialty
\strut\end{minipage} &
\begin{minipage}[t]{0.09\columnwidth}\centering\strut
Nominal
\strut\end{minipage} &
\begin{minipage}[t]{0.35\columnwidth}\centering\strut
Integer identifier of a specialty of the admitting physician,
corresponding to 84 distinct values
\strut\end{minipage} &
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
50\%
\strut\end{minipage}\tabularnewline
\begin{minipage}[t]{0.20\columnwidth}\centering\strut
Diagnosis 3
\strut\end{minipage} &
\begin{minipage}[t]{0.09\columnwidth}\centering\strut
Nominal
\strut\end{minipage} &
\begin{minipage}[t]{0.35\columnwidth}\centering\strut
Additional secondary diagnosis (coded as first three digits of ICD9),
corresponding to 954 distinct values
\strut\end{minipage} &
\begin{minipage}[t]{0.24\columnwidth}\centering\strut
1\%
\strut\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The way I deal with missing values is to delete \texttt{Weight},
\texttt{Payer code} feaures (columns), since both features have more
than 50\% missing values and they are not relevant to classification.
However, \texttt{Medical speciality} was maintained, adding the value
``missing'' in order to account for missing values. (cite)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data[,}\KeywordTok{c}\NormalTok{(-}\DecValTok{6}\NormalTok{,-}\DecValTok{11}\NormalTok{)]}
\NormalTok{data$medical_specialty <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(data$medical_specialty,}
                \DataTypeTok{levels=}\KeywordTok{c}\NormalTok{(}\KeywordTok{levels}\NormalTok{(data$medical_specialty),}\StringTok{'Missing'}\NormalTok{))}
\NormalTok{data[,}\DecValTok{10}\NormalTok{][}\KeywordTok{is.na}\NormalTok{(data$medical_specialty)] <-}\StringTok{ 'Missing'}
\NormalTok{data <-}\StringTok{ }\KeywordTok{na.omit}\NormalTok{(data)}
\KeywordTok{dim}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 98053    48
\end{verbatim}

In general, should split the original data into training and testing
first and then deal with the missing values. However, both methods will
yield same dimension of \texttt{data}. For simplicity, deal with NAs
first here.

\section{Preprocessing}\label{preprocessing}

(Preprocessing is more crucial when using model based algorithms,
e.g.~Linear Discrimant Analysis, Naive Bayes, Linear
Regression\ldots{}than using non-parametrical algorithms.)

\begin{itemize}
\item
  There are also other ways to impute data, like knnimpute\ldots{}For
  convenience, try omitting NA rows first.
\item
  The data has been preprocessed to ensure each encounter has a unique
  id. However, each patient may have more than one id.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(data$encounter_id))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 98053
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(}\KeywordTok{unique}\NormalTok{(data$patient_nbr))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 68630
\end{verbatim}

We thus used only one encounter per patient; in particular, we
considered only the first encounter for each patient as the primary
admission and determined whether or not they were readmitted within 30
days. (cite)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data[}\KeywordTok{order}\NormalTok{(data[,}\DecValTok{2}\NormalTok{]),]}
\NormalTok{unique_list <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\OtherTok{TRUE}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(data))}
\NormalTok{for (l in }\DecValTok{2}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(data))\{}
    \NormalTok{if (data[l,}\StringTok{'patient_nbr'}\NormalTok{]==data[l}\DecValTok{-1}\NormalTok{,}\StringTok{'patient_nbr'}\NormalTok{])\{}
        \NormalTok{unique_list[l]=}\OtherTok{FALSE}
    \NormalTok{\}}
\NormalTok{\}}
\NormalTok{data <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data,unique_list)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Kill the first two features(\texttt{encounter\_id} and
  \texttt{patient\_nbr}) which are ids for encounted and patients.
  Besides, extract label column, which is the last column of
  \texttt{data}. For two-stage classification, generate new label
  \texttt{label\_gen} which has two levels.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data <-}\StringTok{ }\NormalTok{data[,}\KeywordTok{c}\NormalTok{(-}\DecValTok{1}\NormalTok{,-}\DecValTok{2}\NormalTok{)]}
\NormalTok{label <-}\StringTok{ }\NormalTok{data$readmitted}
\NormalTok{label_gen <-}\StringTok{ }\NormalTok{label}
\KeywordTok{levels}\NormalTok{(label_gen) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'Readmission'}\NormalTok{,}\StringTok{'Readmission'}\NormalTok{,}\StringTok{'NO'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Split this train data into training and test dataset according to the
  ratio 6:4 (set seed to make partioning reproducible) (Here I don't
  split validation since without looking data, there is possibility that
  test and training may not have the same distribution of different
  classes. Using cross validation is better in this case, though the
  computational complexity is high.)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inTrain <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(label,}\DataTypeTok{p=}\FloatTok{0.8}\NormalTok{,}\DataTypeTok{list=}\OtherTok{FALSE}\NormalTok{)}
\NormalTok{training <-}\StringTok{ }\NormalTok{data[inTrain,]}
\NormalTok{training_label <-}\StringTok{ }\NormalTok{label[inTrain]}
\NormalTok{training_label_gen <-}\StringTok{ }\NormalTok{label_gen[inTrain]}
\NormalTok{test <-}\StringTok{ }\NormalTok{data[-inTrain,]}
\NormalTok{test_label <-}\StringTok{ }\NormalTok{label[-inTrain]}
\NormalTok{test_label_gen <-}\StringTok{ }\NormalTok{label_gen[-inTrain]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Now, let's see if samples of different classes of \texttt{training}
  dataset are unbalanced.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{,}\DataTypeTok{pin=}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{((training_label), }\DataTypeTok{xlab =} \StringTok{'Days to inpatient readmission'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'frequency'}\NormalTok{,}\DataTypeTok{col =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{main =} \StringTok{'Histogram of different classes'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Project_files/figure-latex/Preprocessing_classtype-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{par}\NormalTok{(}\DataTypeTok{cex=}\FloatTok{0.7}\NormalTok{,}\DataTypeTok{pin=}\KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\KeywordTok{plot}\NormalTok{((training_label_gen), }\DataTypeTok{xlab =} \StringTok{'Readmission or not'}\NormalTok{, }\DataTypeTok{ylab =} \StringTok{'frequency'}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{'red'}\NormalTok{, }\DataTypeTok{main =} \StringTok{'Histogram of different classes'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Project_files/figure-latex/Preprocessing_classtype-2.pdf}

Classes are unbalanced distributed in three levels, but not very skewed.
Since \texttt{\textless{}30} and \texttt{\textgreater{}30} are subsets
of \texttt{Readmission}, hierarchical classification (two-stage
classification) is straightforward.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12121}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Kill unimportant features.(\emph{nearZeroVar} diagnoses predictors
  that have one unique value (i.e.~are zero variance predictors) or
  predictors that are have both of the following characteristics: they
  have very few unique values relative to the number of samples and the
  ratio of the frequency of the most common value to the frequency of
  the second most common value is large.) (cite)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NZV <-}\StringTok{ }\KeywordTok{nearZeroVar}\NormalTok{(training,}\DataTypeTok{saveMetrics =} \NormalTok{T,}\DataTypeTok{freqCut=}\DecValTok{95}\NormalTok{/}\DecValTok{5}\NormalTok{)}
\NormalTok{training <-}\StringTok{ }\NormalTok{training[,-}\KeywordTok{which}\NormalTok{(NZV$nzv==}\OtherTok{TRUE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Kill same features for \texttt{test} dataset without looking inside.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test <-}\StringTok{ }\NormalTok{test[,-}\KeywordTok{which}\NormalTok{(NZV$nzv==}\OtherTok{TRUE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Use background information for all features to analyze which features
  should be converted to numerical features. (cite)
\item
  Note: Need to combine training and test to a big dataset and then
  convert to numerical fearutes. If convert seperately, some feature may
  have some level which has only few points and are all assigned into
  \texttt{training} or \texttt{test} dataset. This may lead to different
  dimensions for \texttt{training} and \texttt{test} dataset after
  conversion.
\item
  First, drop large factors \texttt{diag2} and \texttt{diag3}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training <-}\StringTok{ }\NormalTok{training[,}\KeywordTok{c}\NormalTok{(-}\DecValTok{16}\NormalTok{,-}\DecValTok{17}\NormalTok{)]}
\NormalTok{test <-}\StringTok{ }\NormalTok{test[,}\KeywordTok{c}\NormalTok{(-}\DecValTok{16}\NormalTok{,-}\DecValTok{17}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Second, drop last nine features.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training <-}\StringTok{ }\NormalTok{training[,-}\KeywordTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{:}\DecValTok{25}\NormalTok{)]}
\NormalTok{test <-}\StringTok{ }\NormalTok{test[,-}\KeywordTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{:}\DecValTok{25}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Third, convert categorical features to numerical features.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\StringTok{'~/Documents/2015Fall/EE660/EE660_Project/C2N.R'}\NormalTok{)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(training,test)}
\NormalTok{for (i in }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{15}\NormalTok{))\{}
    \NormalTok{temp <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(data[,i])}
    \NormalTok{data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(data,}\KeywordTok{C2N}\NormalTok{(temp))}
\NormalTok{\}}
\NormalTok{data <-}\StringTok{ }\NormalTok{data[,-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{,}\DecValTok{8}\NormalTok{,}\DecValTok{15}\NormalTok{)]}
\NormalTok{training <-}\StringTok{ }\NormalTok{data[}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(training_label),]}
\NormalTok{test <-}\StringTok{ }\NormalTok{data[(}\KeywordTok{length}\NormalTok{(training_label)+}\DecValTok{1}\NormalTok{):}\DecValTok{68630}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Again, kill unimportant features using \emph{nearZeroVar}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NZV_2 <-}\StringTok{ }\KeywordTok{nearZeroVar}\NormalTok{(training,}\DataTypeTok{saveMetrics =} \NormalTok{T,}\DataTypeTok{freqCut=}\DecValTok{95}\NormalTok{/}\DecValTok{5}\NormalTok{)}
\NormalTok{training <-}\StringTok{ }\NormalTok{training[,-}\KeywordTok{which}\NormalTok{(NZV_2$nzv==}\OtherTok{TRUE}\NormalTok{)]}
\NormalTok{test<-}\StringTok{ }\NormalTok{test[,-}\KeywordTok{which}\NormalTok{(NZV_2$nzv==}\OtherTok{TRUE}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Convert dataframe to numericl matrix for PCA analysis. (Need to use
  validation set to check what is the best dimension in classification.)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(training,}\DecValTok{2}\NormalTok{,as.numeric)}
\NormalTok{test <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(test,}\DecValTok{2}\NormalTok{,as.numeric) }\CommentTok{# workspace saved here}
\NormalTok{PCA_fit <-}\StringTok{ }\KeywordTok{prcomp}\NormalTok{(training)}
\NormalTok{training_std <-}\StringTok{ }\NormalTok{PCA_fit$x[,}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\NormalTok{test_std <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(PCA_fit,test)[,}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Normalize data (There are lots of diffeent methods for normalization,
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training_std <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(training,}\DecValTok{2}\NormalTok{,function(x) (x-}\KeywordTok{mean}\NormalTok{(x))/}\KeywordTok{sd}\NormalTok{(x))}
\NormalTok{test_std <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(test,}\DecValTok{2}\NormalTok{,function(x) (x-}\KeywordTok{mean}\NormalTok{(x))/}\KeywordTok{sd}\NormalTok{(x))}
\end{Highlighting}
\end{Shaded}

\section{Save \texttt{training} and \texttt{test} into csv file for
future use}\label{save-training-and-test-into-csv-file-for-future-use}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write.csv}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(training_std,training_label),}\StringTok{'training.csv'}\NormalTok{,}\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{write.csv}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(test_std,test_label),}\StringTok{'test.csv'}\NormalTok{,}\DataTypeTok{row.names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Purpose for doing that is \texttt{R} is good for exploratory research,
but not good at dealing with large dataset for clasification. Use
\texttt{Python} to read csv as \texttt{SFrame} for classification will
speed up! (cite)

\end{document}
